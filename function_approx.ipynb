{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class U_net(nn.Module):\n",
    "    def __init__(self, layers, params):\n",
    "        super(U_net, self).__init__()\n",
    "        self.params = params  \n",
    "        \n",
    "        # Define your network layers here\n",
    "        net_layers = []\n",
    "\n",
    "        for i in range(1, len(layers)):\n",
    "            linear_layer = nn.Linear(layers[i-1], layers[i])\n",
    "            torch.nn.init.xavier_uniform_(self.linear_layer.weight)\n",
    "            net_layers.append(linear_layer)\n",
    "            if i < len(layers) - 1:\n",
    "                net_layers.append(nn.ReLU())\n",
    "\n",
    "        self.U = nn.Sequential(*net_layers)\n",
    "\n",
    "    def F(self,x_f, t_f):\n",
    "        # Define your additional operations here\n",
    "        U_t = torch.autograd.grad(self.U_inner, t_f,torch.ones_like(t_f),create_graph=True)[0]  #ones_like(t_f)使得能计算每一对U_inner和t_f的导数\n",
    "        \n",
    "        U_x = torch.autograd.grad(self.U_inner, x_f, torch.ones_like(x_f),create_graph=True)[0]\n",
    "        \n",
    "        U_xx = torch.autograd.grad(U_x, x_f, torch.ones_like(x_f),create_graph=True)[0]\n",
    "       \n",
    "        return U_t + self.U_inner * U_x - self.params * U_xx\n",
    "    \n",
    "\n",
    "    def forward(self, x_u, t_u, x_f, t_f):\n",
    "       \n",
    "        self.U_bound = self.U(torch.cat((x_u, t_u), 1))\n",
    "        self.U_inner = self.U(torch.cat((x_f, t_f), 1))\n",
    "        F_output = self.F(x_f, t_f)  # Calculate the result of your additional operations\n",
    "\n",
    "        return F_output, self.U_bound  # Return both F_output and U_output\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from pyDOE import lhs\n",
    "data = scipy.io.loadmat(r'burgers_shock.mat')\n",
    "\n",
    "N_u = 100\n",
    "N_f = 10000\n",
    "params = 0.01/np.pi\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))  #（x,t）的所有组合\n",
    "u_star = Exact.flatten()[:,None]              \n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)  #（x,t）的最小值\n",
    "ub = X_star.max(0)  #（x,t）的最大值\n",
    "    \n",
    "xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T)) #X最小值的边界\n",
    "uu1 = Exact[0:1,:].T\n",
    "xx2 = np.hstack((X[:,0:1], T[:,0:1]))  #T最小值的边界\n",
    "uu2 = Exact[:,0:1]\n",
    "xx3 = np.hstack((X[:,-1:], T[:,-1:]))  #T最大值的边界\n",
    "uu3 = Exact[:,-1:]\n",
    "\n",
    "X_u_train = np.vstack([xx1, xx2, xx3])  #边界点纵向堆叠\n",
    "X_f_train = lb + (ub-lb)*lhs(2, N_f)  #lhs生成N_f行，2列的随机数,取得Nf个内部点\n",
    "#X_f_train  =lb + (ub-lb)*np.random.rand(2,N_f)\n",
    "X_f_train = np.vstack((X_f_train, X_u_train))\n",
    "u_train = np.vstack([uu1, uu2, uu3])\n",
    "\n",
    "idx = np.random.choice(X_u_train.shape[0], N_u, replace=False) #从X_u_train中随机选取N_u个点\n",
    "X_u_train = X_u_train[idx, :]\n",
    "u_train = u_train[idx,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of your U_net model\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "model = U_net(layers, params)\n",
    "\n",
    "# Input data\n",
    "x_bound = torch.Tensor(X_u_train[:,0:1])\n",
    "\n",
    "t_bound = torch.Tensor(X_u_train[:,1:2])\n",
    "\n",
    "x_inner = torch.Tensor(X_f_train[:,0:1])\n",
    "x_inner.requires_grad=True\n",
    "\n",
    "t_inner = torch.Tensor(X_f_train[:,1:2])\n",
    "t_inner.requires_grad=True\n",
    "u_train_tf = torch.Tensor(u_train)\n",
    "\n",
    "#Forward pass through the model\n",
    "\n",
    "# Now you can use F_output and U_output in your loss function\n",
    "\n",
    "# Define an optimizer\n",
    "\n",
    "def loss_func(F, U):\n",
    "    return torch.mean(torch.square(F)) + torch.mean(torch.square(U - u_train_tf))\n",
    "\n",
    "# def closure():\n",
    "#     optimizer.zero_grad()\n",
    "#     F_output, U_output = model(x_bound, t_bound, x_inner, t_inner)\n",
    "#     loss = loss_func(F_output, U_output)\n",
    "#     loss.backward()\n",
    "#     return loss\n",
    "\n",
    "#Training loop\n",
    "# num_epochs = 1000\n",
    "#optimizer = optim.LBFGS(model.parameters(),max_iter=50000,)\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Forward pass\n",
    "#     # Calculate the loss\n",
    "#     def closure():\n",
    "#         optimizer.zero_grad()\n",
    "#         F_output, U_output = model(x_bound, t_bound, x_inner, t_inner)\n",
    "#         loss = loss_func(F_output, U_output)\n",
    "#         loss.backward()\n",
    "#         return loss\n",
    "\n",
    "#     optimizer.step(closure)\n",
    "#     if (epoch + 1) % 1 == 0:\n",
    "#         loss = closure()\n",
    "#         print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "optimizer = optim.adam(model.parameters(), lr=0.01)\n",
    "num_epochs = 100000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    # Calculate the loss\n",
    "    F_output, U_output = model(x_bound, t_bound, x_inner, t_inner)\n",
    "    loss = loss_func(F_output, U_output)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 1e4 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "from scipy.interpolate import griddata\n",
    "pred_x = torch.tensor(X_star[:,0:1],dtype=torch.float32).to('cuda')\n",
    "pred_x.requires_grad=True\n",
    "pred_t = torch.tensor(X_star[:,1:2],dtype=torch.float32).to('cuda')\n",
    "pred_t.requires_grad=True\n",
    "\n",
    "F_pred, U_pred = model(pred_x, pred_t, pred_x, pred_t) #计算相同点的F和U\n",
    "#convert to numpy\n",
    "U_pred = U_pred.detach().numpy()\n",
    "U_interp = griddata(X_star, U_pred.flatten(), (X, T), method='cubic')  #把一列重新整理为矩阵\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Exact.T, interpolation='nearest', cmap='rainbow',extent=[t.min(), t.max(), x.min(), x.max()], origin='lower')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.imshow(U_interp.T,interpolation='nearest', origin='lower', cmap='rainbow',extent=[lb[1], ub[1], lb[0], ub[0]])\n",
    "\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
